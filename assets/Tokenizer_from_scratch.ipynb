{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Tokenizer from scratch\n",
        "- Author: Parvesh\n",
        "\n"
      ],
      "metadata": {
        "id": "K4xyobEpdkE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets start by dowloading the lib**"
      ],
      "metadata": {
        "id": "O0vwFs5bd8lg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gE-Ez1qtyIA"
      },
      "outputs": [],
      "source": [
        "# this will dowload the latest commit from the gihub to your colab vm\n",
        "!pip install git+https://github.com/OE-Void/Tokenizer-from_scratch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if False: # Change it to True is you want to train a TikToken tokenizer\n",
        "  from TikToken.tok import Trainer\n",
        "  from TikToken.data import batch_iterator\n",
        "if False: # Change it to True is you want to train a BPE tokenizer\n",
        "  from BPE.tok import Trainer\n",
        "  from BPE.data import batch_iterator # loads the default data pipeline\n"
      ],
      "metadata": {
        "id": "-eUeNa0rKOkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Diff btw TikToken, BPE**\n",
        "- BPE: A tokenizer framework by Hugging Face using Byte Pair Encoding, written in Rust and highly customizable.\n",
        "\n",
        "- TikToken: A Rust-based BPE tokenizer by OpenAI, optimized for speed and compatibility with GPT models — often 5–6× faster than standard Hugging Face BPE implementations."
      ],
      "metadata": {
        "id": "d6clG6fDe5pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets setup the data-pipeline first\n",
        "if False:\n",
        "  batch_iter = batch_iterator() # default trains the tokenizer on 10b tokens\n",
        "\n",
        "if False:\n",
        "  batch_iter = batch_iterator( # this is the default used if you are confortable with it you can go with the upper as both have excat same arguments\n",
        "      BATCH_SIZE = 10_000,\n",
        "      Dataset=\"HuggingFaceFW/fineweb-edu\",\n",
        "      split=\"train\",\n",
        "      name=\"sample-10BT\", # you can chage the name from the below but the 10b tokens config is enough\n",
        "      \"\"\"\n",
        "      sample-350BT: a subset randomly sampled from the whole dataset of around 350B gpt2 tokens\n",
        "      sample-100BT: a subset randomly sampled from the whole dataset of around 100B gpt2 tokens\n",
        "      sample-10BT: a subset randomly sampled from the whole dataset of around 10B gpt2 tokens # used in default config\n",
        "      \"\"\"\n",
        "      streaming=True,\n",
        "      trust_remote_code=True\n",
        "      )\n"
      ],
      "metadata": {
        "id": "kI-VmAO5fU1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trainer**"
      ],
      "metadata": {
        "id": "SPaFWUeTgnWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to use default config use\n",
        "if False:\n",
        "  Trainer(batch_iter)\n",
        "\n",
        "# to use custom config\n",
        "\n",
        "if False:\n",
        "  Trainer(\n",
        "      batch_iterator,\n",
        "      vocab_size=64_000,\n",
        "      special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "      save_to=\"tokenizer_tiktoken\" # saves the Tokenizer files\n",
        "  )"
      ],
      "metadata": {
        "id": "HfPIhQYvgnDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the saved tokenizer**"
      ],
      "metadata": {
        "id": "CiWHsDSEkJKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizer_tiktoken\") # change the folder name if save_to was chaged\n",
        "text = \"Hello, world! This is a test.\"\n",
        "encoded = tokenizer(text)\n",
        "print(f\"Tokens: {encoded.tokens()}\")\n",
        "print(f\"IDs: {encoded['input_ids']}\")\n",
        "\n",
        "decoded = tokenizer.decode(encoded['input_ids'])\n",
        "print(f\"Decoded: {decoded}\")"
      ],
      "metadata": {
        "id": "kHsW0LVSkI4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thank you**"
      ],
      "metadata": {
        "id": "ThgZBm23hOrT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}